{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将数据分组,并用软连接表示\n",
    "- 原始数据 25000\n",
    "- divide in to\n",
    "- train\n",
    " - dogs[1000]\n",
    " - cats[1000]\n",
    "- validation\n",
    " - dogs[400]\n",
    " - cats[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import shutil\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#seed \n",
    "np.random.seed(2018)\n",
    "\n",
    "source_dir = '/home/mxq/Jupyter/Keras/dogsvscats/data/train/'\n",
    "#target_dir = '/home/mxq/Jupyter/Keras/dogsvscats/data/train_sym'\n",
    "target_dir = '/home/mxq/Jupyter/Keras/dogsvscats/data/test_sym'\n",
    "def datadivide():\n",
    "    samples,classes = generate_valid_imagepaths(source_dir)\n",
    "    for _ in classes.items():\n",
    "        #shuffle(_[1])\n",
    "        classes[_[0]] = _[1][2000:2500]\n",
    "    dothings(classes)\n",
    "    print('done')\n",
    "def dothings(classes):\n",
    "    for class_path in classes.items():\n",
    "        sub_dir = os.path.join(target_dir,class_path[0])\n",
    "        if os.path.exists(sub_dir):\n",
    "            shutil.rmtree(sub_dir)\n",
    "        os.mkdir(sub_dir)\n",
    "        filepaths = class_path[1]\n",
    "        symlink_img(filepaths,sub_dir)\n",
    "\n",
    "def symlink_img(filepaths,target_dir):\n",
    "    for source_filepath in filepaths:\n",
    "        symfilepath = os.path.split(source_filepath)[-1]\n",
    "        target_filepath = os.path.join(target_dir,symfilepath)\n",
    "        os.symlink(source_filepath,target_filepath)\n",
    "    \n",
    "def generate_valid_imagepaths(directory,follow_links=False):\n",
    "    \n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath,followlinks=follow_links),key=lambda tpl:tpl[0])\n",
    "    samples = 0\n",
    "    classes_file = defaultdict(list)\n",
    "    for dirpath, _, files in _recursive_list(directory):\n",
    "        for fname in sorted(files,key=lambda s:int(s[4:-4])):\n",
    "        #for fname in sorted(files):\n",
    "            if fname[:3] == 'dog':\n",
    "               filepath = os.path.join(dirpath, fname)\n",
    "               classes_file['dogs'].append(filepath)\n",
    "               samples += 1\n",
    "            else:\n",
    "                filepath = os.path.join(dirpath, fname)\n",
    "                classes_file['cats'].append(filepath)\n",
    "                samples += 1\n",
    "                \n",
    "    return samples,classes_file\n",
    "if __name__ == '__main__':\n",
    "    datadivide()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array,load_img\n",
    "import shutil\n",
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "img = load_img('./data/train_sym/cats/cat.1.jpg')\n",
    "x = img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "i = 0\n",
    "\n",
    "if os.path.exists('data/preview/'):\n",
    "    shutil.rmtree('data/preview/')\n",
    "os.mkdir('data/preview/')\n",
    "for batch in datagen.flow(x,batch_size=1,save_to_dir='data/preview',save_prefix='cat',save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.layers import Activation,Dropout,Flatten,Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(150,150,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从文件夹中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Let's prepare our data. We will use .flow_from_directory() to generate batches \n",
    "#of image data (and their labels) directly from our jpgs in their respective folders.\n",
    "batch_size = 16\n",
    "# This is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "# This is the augmentation configuration we will use for testing:\n",
    "# Only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# This is a generator that will read pictures found in subfolder of 'data/train_sym',\n",
    "# and indefinitely generate batches of augmented image data.\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                  'data/train_sym/',\n",
    "                   target_size=(150,150),\n",
    "                   batch_size=batch_size,\n",
    "                   class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                   'data/validation_sym/',\n",
    "                    target_size=(150,150),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 57s 453ms/step - loss: 0.7032 - acc: 0.5165 - val_loss: 0.6715 - val_acc: 0.5687\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 57s 457ms/step - loss: 0.6812 - acc: 0.5640 - val_loss: 0.6496 - val_acc: 0.6350\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.6529 - acc: 0.6230 - val_loss: 0.6526 - val_acc: 0.5950\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 57s 453ms/step - loss: 0.6311 - acc: 0.6480 - val_loss: 0.5919 - val_acc: 0.6925\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.6108 - acc: 0.6810 - val_loss: 0.5702 - val_acc: 0.6987\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.5962 - acc: 0.7040 - val_loss: 0.5455 - val_acc: 0.7000\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 55s 439ms/step - loss: 0.5630 - acc: 0.7205 - val_loss: 0.5663 - val_acc: 0.6913\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.5910 - acc: 0.7215 - val_loss: 0.5694 - val_acc: 0.7200\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.5462 - acc: 0.7520 - val_loss: 0.5493 - val_acc: 0.7338\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 57s 452ms/step - loss: 0.5432 - acc: 0.7450 - val_loss: 0.5484 - val_acc: 0.7412\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.5146 - acc: 0.7530 - val_loss: 0.5202 - val_acc: 0.7362\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 56s 447ms/step - loss: 0.5073 - acc: 0.7580 - val_loss: 0.4961 - val_acc: 0.7738\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 56s 448ms/step - loss: 0.5089 - acc: 0.7650 - val_loss: 0.5023 - val_acc: 0.7488\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 56s 448ms/step - loss: 0.4921 - acc: 0.7780 - val_loss: 0.4872 - val_acc: 0.7512\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 0.4672 - acc: 0.7960 - val_loss: 0.6035 - val_acc: 0.7550\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 0.4616 - acc: 0.7840 - val_loss: 0.5212 - val_acc: 0.7612\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 0.4649 - acc: 0.7905 - val_loss: 0.5144 - val_acc: 0.7863\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 0.4660 - acc: 0.7945 - val_loss: 0.5396 - val_acc: 0.7588\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 56s 450ms/step - loss: 0.4558 - acc: 0.8010 - val_loss: 0.5925 - val_acc: 0.7238\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 56s 450ms/step - loss: 0.4383 - acc: 0.8020 - val_loss: 0.5111 - val_acc: 0.7788\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 56s 451ms/step - loss: 0.4533 - acc: 0.7985 - val_loss: 0.4874 - val_acc: 0.7712\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 56s 452ms/step - loss: 0.4279 - acc: 0.8120 - val_loss: 0.6113 - val_acc: 0.7700\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 56s 451ms/step - loss: 0.4232 - acc: 0.8140 - val_loss: 0.5956 - val_acc: 0.7638\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 56s 450ms/step - loss: 0.4098 - acc: 0.8210 - val_loss: 0.4801 - val_acc: 0.7688\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 56s 452ms/step - loss: 0.4242 - acc: 0.8095 - val_loss: 0.5451 - val_acc: 0.7700\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.4343 - acc: 0.8145 - val_loss: 0.4823 - val_acc: 0.7775\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.4311 - acc: 0.8180 - val_loss: 0.5821 - val_acc: 0.7762\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.4128 - acc: 0.8170 - val_loss: 0.5013 - val_acc: 0.7875\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.4251 - acc: 0.8190 - val_loss: 0.5716 - val_acc: 0.7538\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.4111 - acc: 0.8195 - val_loss: 0.5166 - val_acc: 0.7762\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 55s 441ms/step - loss: 0.4187 - acc: 0.8220 - val_loss: 0.6351 - val_acc: 0.7688\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.4236 - acc: 0.8195 - val_loss: 0.6485 - val_acc: 0.7600\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.4104 - acc: 0.8310 - val_loss: 0.7080 - val_acc: 0.7588\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 55s 441ms/step - loss: 0.4141 - acc: 0.8355 - val_loss: 0.5521 - val_acc: 0.7887\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 55s 443ms/step - loss: 0.3904 - acc: 0.8325 - val_loss: 0.5076 - val_acc: 0.8025\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.3942 - acc: 0.8305 - val_loss: 0.5372 - val_acc: 0.7638\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 0.4016 - acc: 0.8305 - val_loss: 0.4742 - val_acc: 0.7875\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 56s 444ms/step - loss: 0.4043 - acc: 0.8300 - val_loss: 0.5890 - val_acc: 0.7688\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 0.3904 - acc: 0.8350 - val_loss: 0.5498 - val_acc: 0.8050\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.3805 - acc: 0.8490 - val_loss: 0.7018 - val_acc: 0.7638\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 0.3823 - acc: 0.8410 - val_loss: 0.7005 - val_acc: 0.7675\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.3689 - acc: 0.8375 - val_loss: 0.6397 - val_acc: 0.7450\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.4054 - acc: 0.8345 - val_loss: 0.5153 - val_acc: 0.7438\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 55s 436ms/step - loss: 0.3727 - acc: 0.8410 - val_loss: 0.6298 - val_acc: 0.7512\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 55s 436ms/step - loss: 0.4211 - acc: 0.8370 - val_loss: 0.5492 - val_acc: 0.7725\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.3809 - acc: 0.8475 - val_loss: 0.9736 - val_acc: 0.7688\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.3889 - acc: 0.8345 - val_loss: 0.5180 - val_acc: 0.7788\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.3870 - acc: 0.8365 - val_loss: 0.8777 - val_acc: 0.7200\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.4075 - acc: 0.8365 - val_loss: 0.6691 - val_acc: 0.8063\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.3733 - acc: 0.8430 - val_loss: 0.5480 - val_acc: 0.7625\n"
     ]
    }
   ],
   "source": [
    "'''model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights('first_try.h5')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#seed\n",
    "np.random.seed(2018)\n",
    "test_batch_size = 16\n",
    "pre_gen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = pre_gen.flow_from_directory('data/train_sym',(150,150),\n",
    "                                         class_mode='binary',shuffle=False,batch_size=test_batch_size)\n",
    "steps_test = test_generator.samples\n",
    "\n",
    "model.load_weights('first_try.h5')\n",
    "#pred = model.predict_generator(test_generator,steps=steps_test)\n",
    "#output = open(\"pred.npy\",'wb')\n",
    "#np.save(output,pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.569524]\n",
      "[0.24353099]\n",
      "[0.6307373]\n",
      "[2.4851664e-05]\n",
      "[0.14670713]\n",
      "[0.02440804]\n",
      "[0.0080556]\n",
      "[0.11119525]\n",
      "[0.00990648]\n",
      "[0.5032455]\n",
      "[0.4362974]\n",
      "[0.10433654]\n",
      "[0.09692132]\n",
      "[0.09082355]\n",
      "[0.24296084]\n",
      "[0.21188118]\n",
      "[0.02284339]\n",
      "[0.09276731]\n",
      "[0.15496062]\n",
      "[0.28689238]\n",
      "[0.27981094]\n",
      "[0.01421825]\n",
      "[0.03546935]\n",
      "[0.13426065]\n",
      "[0.02428766]\n",
      "[0.3039123]\n",
      "[0.2389768]\n",
      "[0.24949487]\n",
      "[0.25882527]\n",
      "[0.17203727]\n",
      "[0.03869753]\n",
      "[0.27117956]\n",
      "[0.01065296]\n",
      "[0.13077316]\n",
      "[0.02419917]\n",
      "[0.00784016]\n",
      "[0.06829943]\n",
      "[0.3648785]\n",
      "[0.69841176]\n",
      "[0.04894631]\n",
      "[0.08862553]\n",
      "[0.00248957]\n",
      "[0.03965409]\n",
      "[0.03877462]\n",
      "[0.4040083]\n",
      "[0.3375469]\n",
      "[0.11871459]\n",
      "[0.10982618]\n",
      "[0.29416734]\n",
      "[0.24238849]\n",
      "[0.12542868]\n",
      "[0.4763415]\n",
      "[0.23393305]\n",
      "[0.27782378]\n",
      "[0.08377693]\n",
      "[0.1516005]\n",
      "[0.2945772]\n",
      "[0.10152912]\n",
      "[0.07677276]\n",
      "[0.17758442]\n",
      "[0.09594364]\n",
      "[0.7177048]\n",
      "[0.00453952]\n",
      "[0.9781145]\n",
      "[0.00463718]\n",
      "[0.18255132]\n",
      "[0.1546647]\n",
      "[0.15818666]\n",
      "[0.00682027]\n",
      "[0.09713107]\n",
      "[0.21686982]\n",
      "[0.53013295]\n",
      "[0.23304376]\n",
      "[0.0779893]\n",
      "[0.38681173]\n",
      "[0.22856914]\n",
      "[0.03384158]\n",
      "[0.15612671]\n",
      "[0.36598304]\n",
      "[0.9958259]\n",
      "[0.20973085]\n",
      "[0.00041558]\n",
      "[0.28148305]\n",
      "[0.14239295]\n",
      "[0.3815536]\n",
      "[0.5354309]\n",
      "[0.02589076]\n",
      "[0.06956343]\n",
      "[0.05653171]\n",
      "[0.0810506]\n",
      "[0.29915673]\n",
      "[0.07564375]\n",
      "[0.06305913]\n",
      "[0.20277531]\n",
      "[0.07450419]\n",
      "[0.37391827]\n",
      "[0.15681322]\n",
      "[0.28928486]\n",
      "[0.05031253]\n",
      "[0.04067373]\n",
      "[0.12759222]\n",
      "[0.3337715]\n",
      "[0.00483212]\n",
      "[0.14423813]\n",
      "[0.0959141]\n",
      "[0.2582736]\n",
      "[0.00413435]\n",
      "[0.02488837]\n",
      "[0.0116002]\n",
      "[0.56375724]\n",
      "[0.01876781]\n",
      "[0.5219372]\n",
      "[0.00911508]\n",
      "[0.02302896]\n",
      "[0.00024218]\n",
      "[0.01371472]\n",
      "[0.399497]\n",
      "[0.05426189]\n",
      "[0.40591082]\n",
      "[0.02870635]\n",
      "[0.3295122]\n",
      "[0.14010923]\n",
      "[0.04033685]\n",
      "[0.11963837]\n",
      "[0.00198191]\n",
      "[0.3514828]\n",
      "[0.00082011]\n",
      "[0.5812719]\n",
      "[0.03992522]\n",
      "[0.07378613]\n",
      "[0.06141006]\n",
      "[0.04736888]\n",
      "[0.29591623]\n",
      "[0.0883186]\n",
      "[0.01520895]\n",
      "[0.06431109]\n",
      "[0.00100988]\n",
      "[0.00542223]\n",
      "[0.04246814]\n",
      "[2.9481705e-05]\n",
      "[0.03235731]\n",
      "[0.2703081]\n",
      "[0.07997905]\n",
      "[0.4736044]\n",
      "[0.18648076]\n",
      "[0.16857836]\n",
      "[0.09089715]\n",
      "[0.1267362]\n",
      "[0.0553588]\n",
      "[0.00470093]\n",
      "[0.07537029]\n",
      "[0.01248782]\n",
      "[0.7490566]\n",
      "[0.16979356]\n",
      "[0.11287614]\n",
      "[0.5505229]\n",
      "[0.27942178]\n",
      "[0.1462246]\n",
      "[0.7052727]\n",
      "[0.6512522]\n",
      "[0.00152164]\n",
      "[0.34607932]\n",
      "[0.28216097]\n",
      "[0.03504676]\n",
      "[0.14435494]\n",
      "[0.20908906]\n",
      "[0.5805702]\n",
      "[0.00011614]\n",
      "[0.6976282]\n",
      "[0.00791805]\n",
      "[0.27141213]\n",
      "[0.22638187]\n",
      "[0.06695697]\n",
      "[0.07927699]\n",
      "[0.23837595]\n",
      "[0.1437637]\n",
      "[0.2525872]\n",
      "[0.6383334]\n",
      "[0.39429122]\n",
      "[0.342153]\n",
      "[0.00700861]\n",
      "[0.02205911]\n",
      "[0.98717946]\n",
      "[0.3697526]\n",
      "[0.0949536]\n",
      "[0.11083849]\n",
      "[0.33968884]\n",
      "[0.02711471]\n",
      "[0.17521454]\n",
      "[0.20702042]\n",
      "[0.21916668]\n",
      "[0.24172005]\n",
      "[0.00010702]\n",
      "[0.01329935]\n",
      "[0.0116288]\n",
      "[0.39961118]\n",
      "[0.27207765]\n",
      "[0.00022146]\n",
      "[0.19889976]\n",
      "[0.0101789]\n",
      "[0.18084471]\n",
      "[0.09658075]\n",
      "[0.25055832]\n",
      "[0.24345852]\n",
      "[0.19310181]\n",
      "[0.84885865]\n",
      "[0.0427392]\n",
      "[0.0177574]\n",
      "[0.4508997]\n",
      "[0.04326687]\n",
      "[0.01107283]\n",
      "[0.52791685]\n",
      "[0.00043517]\n",
      "[0.2070431]\n",
      "[0.21864761]\n",
      "[0.23489751]\n",
      "[0.65688246]\n",
      "[0.11659601]\n",
      "[0.4278684]\n",
      "[0.00308337]\n",
      "[0.1109118]\n",
      "[0.04045864]\n",
      "[0.29174924]\n",
      "[0.0014291]\n",
      "[0.08437788]\n",
      "[0.3226146]\n",
      "[0.2000306]\n",
      "[0.02771034]\n",
      "[0.05296703]\n",
      "[0.13998576]\n",
      "[0.12736295]\n",
      "[0.10627808]\n",
      "[0.5153419]\n",
      "[0.15881276]\n",
      "[0.12239948]\n",
      "[0.07464737]\n",
      "[0.01622395]\n",
      "[0.4185764]\n",
      "[0.1672328]\n",
      "[0.00037297]\n",
      "[0.01092767]\n",
      "[0.2513782]\n",
      "[0.02670173]\n",
      "[0.2729668]\n",
      "[0.01026945]\n",
      "[0.35761452]\n",
      "[0.3353165]\n",
      "[0.22589125]\n",
      "[0.5146393]\n",
      "[0.10247418]\n",
      "[0.12816028]\n",
      "[0.6194941]\n",
      "[0.00881451]\n",
      "[0.35296124]\n",
      "[0.19026025]\n",
      "[0.96041703]\n",
      "[0.18845618]\n",
      "[0.10673378]\n",
      "[0.04086411]\n",
      "[0.18490772]\n",
      "[0.44319594]\n",
      "[0.00887062]\n",
      "[0.57713485]\n",
      "[0.13996562]\n",
      "[0.8660232]\n",
      "[0.19097061]\n",
      "[0.0695802]\n",
      "[0.22004846]\n",
      "[0.01313929]\n",
      "[0.545392]\n",
      "[0.505542]\n",
      "[0.00214405]\n",
      "[0.0741226]\n",
      "[0.80763626]\n",
      "[0.2336686]\n",
      "[0.19051102]\n",
      "[0.00931044]\n",
      "[0.03761883]\n",
      "[0.3361429]\n",
      "[0.97220236]\n",
      "[0.04436976]\n",
      "[6.0291506e-05]\n",
      "[0.37915567]\n",
      "[0.22381407]\n",
      "[0.9624619]\n",
      "[0.04838238]\n",
      "[0.21782395]\n",
      "[3.580676e-05]\n",
      "[0.5815855]\n",
      "[0.15297712]\n",
      "[0.0456509]\n",
      "[0.12705693]\n",
      "[0.00040647]\n",
      "[0.00353207]\n",
      "[0.09911662]\n",
      "[0.281456]\n",
      "[0.00196669]\n",
      "[0.33476043]\n",
      "[0.291241]\n",
      "[0.4950978]\n",
      "[0.02706932]\n",
      "[0.35133952]\n",
      "[0.07786696]\n",
      "[0.15497425]\n",
      "[0.02483264]\n",
      "[0.48860174]\n",
      "[0.06975732]\n",
      "[0.17224416]\n",
      "[0.14649956]\n",
      "[0.0392122]\n",
      "[0.11301904]\n",
      "[0.39104977]\n",
      "[0.47802106]\n",
      "[0.209471]\n",
      "[0.0597648]\n",
      "[0.01451629]\n",
      "[0.4520988]\n",
      "[0.56523114]\n",
      "[0.22317973]\n",
      "[0.05404877]\n",
      "[0.44042522]\n",
      "[0.09386645]\n",
      "[0.27589753]\n",
      "[0.02359472]\n",
      "[0.04393165]\n",
      "[0.09109521]\n",
      "[0.1430775]\n",
      "[0.39153114]\n",
      "[0.00052149]\n",
      "[0.04094903]\n",
      "[0.85799414]\n",
      "[0.05361899]\n",
      "[0.1791928]\n",
      "[0.47326168]\n",
      "[0.00284764]\n",
      "[0.1967969]\n",
      "[0.4047989]\n",
      "[0.9508095]\n",
      "[0.17773241]\n",
      "[0.03017838]\n",
      "[0.22015435]\n",
      "[0.32338]\n",
      "[0.12042936]\n",
      "[0.32323113]\n",
      "[0.0240797]\n",
      "[0.07483041]\n",
      "[0.0644138]\n",
      "[0.03439756]\n",
      "[0.30691668]\n",
      "[0.00071822]\n",
      "[0.07402063]\n",
      "[0.4771599]\n",
      "[0.07050931]\n",
      "[0.12429945]\n",
      "[0.08900549]\n",
      "[0.66143095]\n",
      "[0.00011327]\n",
      "[0.11230043]\n",
      "[0.21697451]\n",
      "[0.09597666]\n",
      "[0.03528913]\n",
      "[0.07251006]\n",
      "[0.53647035]\n",
      "[0.15081023]\n",
      "[0.08064578]\n",
      "[0.3864091]\n",
      "[0.00518912]\n",
      "[0.13618363]\n",
      "[0.20717879]\n",
      "[0.0214472]\n",
      "[0.08608344]\n",
      "[0.07560153]\n",
      "[0.07017215]\n",
      "[0.81647706]\n",
      "[0.1622677]\n",
      "[0.0239671]\n",
      "[0.3140085]\n",
      "[0.04032467]\n",
      "[0.00601924]\n",
      "[0.739437]\n",
      "[0.0708217]\n",
      "[0.1291464]\n",
      "[0.12914142]\n",
      "[0.00102828]\n",
      "[0.96991146]\n",
      "[0.06793766]\n",
      "[0.5472738]\n",
      "[0.5708728]\n",
      "[0.81709415]\n",
      "[0.4439685]\n",
      "[0.09437265]\n",
      "[0.9655995]\n",
      "[0.00587112]\n",
      "[0.02329565]\n",
      "[0.1381735]\n",
      "[0.33435386]\n",
      "[0.33095324]\n",
      "[0.2731502]\n",
      "[0.09664591]\n",
      "[0.69914865]\n",
      "[0.02319674]\n",
      "[0.01712057]\n",
      "[0.00797546]\n",
      "[0.29371092]\n",
      "[0.15475835]\n",
      "[0.33617464]\n",
      "[0.04697953]\n",
      "[0.37931556]\n",
      "[0.48964053]\n",
      "[0.00292925]\n",
      "[0.15632962]\n",
      "[0.02882468]\n",
      "[0.01507107]\n",
      "[0.22976586]\n",
      "[0.9893467]\n",
      "[0.8037284]\n",
      "[0.26009747]\n",
      "[0.6407508]\n",
      "[0.57316256]\n",
      "[0.15747318]\n",
      "[0.15344505]\n",
      "[0.04915468]\n",
      "[0.3456724]\n",
      "[0.38111684]\n",
      "[0.04134352]\n",
      "[0.37331954]\n",
      "[0.17456579]\n",
      "[0.00499167]\n",
      "[0.1509642]\n",
      "[0.09158566]\n",
      "[0.00638095]\n",
      "[0.2178678]\n",
      "[0.5376084]\n",
      "[0.11053563]\n",
      "[0.34923467]\n",
      "[0.3429626]\n",
      "[0.2030543]\n",
      "[0.39606443]\n",
      "[0.27097505]\n",
      "[0.07811097]\n",
      "[0.26846823]\n",
      "[0.14286904]\n",
      "[0.38427866]\n",
      "[0.57781905]\n",
      "[0.01332352]\n",
      "[0.01020498]\n",
      "[0.00569652]\n",
      "[0.20496385]\n",
      "[0.07743324]\n",
      "[0.02233188]\n",
      "[0.05650885]\n",
      "[0.07756431]\n",
      "[0.00987989]\n",
      "[0.05119137]\n",
      "[0.20996442]\n",
      "[0.00385228]\n",
      "[0.46302658]\n",
      "[0.50298756]\n",
      "[0.00825863]\n",
      "[0.29387754]\n",
      "[0.2695778]\n",
      "[0.23782092]\n",
      "[0.09549323]\n",
      "[0.00232332]\n",
      "[0.3073059]\n",
      "[0.03015291]\n",
      "[2.2178821e-05]\n",
      "[0.8537582]\n",
      "[0.00066219]\n",
      "[0.12053607]\n",
      "[0.00400707]\n",
      "[0.12159737]\n",
      "[0.98545974]\n",
      "[0.03123773]\n",
      "[0.3700721]\n",
      "[0.47771975]\n",
      "[0.7557595]\n",
      "[0.16563816]\n",
      "[0.00561378]\n",
      "[0.52235293]\n",
      "[0.1435378]\n",
      "[0.07369251]\n",
      "[0.13102582]\n",
      "[0.21615395]\n",
      "[0.02510296]\n",
      "[0.10673844]\n",
      "[0.33787054]\n",
      "[0.03288878]\n",
      "[0.6085676]\n",
      "[0.18983923]\n",
      "[0.00037389]\n",
      "[0.3058165]\n",
      "[0.39739388]\n",
      "[0.01333307]\n",
      "[0.5534346]\n",
      "[0.3975358]\n",
      "[0.0480396]\n",
      "[0.00095111]\n",
      "[0.05289894]\n",
      "[0.02384625]\n",
      "[1.]\n",
      "[0.6075976]\n",
      "[0.48738277]\n",
      "[0.5917133]\n",
      "[0.8347639]\n",
      "[0.35719824]\n",
      "[0.00015868]\n",
      "[0.875841]\n",
      "[0.03632344]\n",
      "[0.06582367]\n",
      "[0.91455764]\n",
      "[0.5571588]\n",
      "[0.99143183]\n",
      "[0.8901075]\n",
      "[0.36790967]\n",
      "[0.43966514]\n",
      "[0.50948584]\n",
      "[0.90047884]\n",
      "[0.2497824]\n",
      "[0.96359444]\n",
      "[0.51652473]\n",
      "[0.39197624]\n",
      "[0.5214504]\n",
      "[0.7972886]\n",
      "[0.6664313]\n",
      "[0.99998367]\n",
      "[0.97997355]\n",
      "[0.6204909]\n",
      "[0.4998421]\n",
      "[0.6505602]\n",
      "[0.6188844]\n",
      "[0.71102804]\n",
      "[0.9787627]\n",
      "[0.79245615]\n",
      "[0.37592685]\n",
      "[0.9994942]\n",
      "[0.9894286]\n",
      "[0.34276736]\n",
      "[0.7913215]\n",
      "[0.99976236]\n",
      "[0.72854316]\n",
      "[0.5627974]\n",
      "[0.35676244]\n",
      "[0.7594889]\n",
      "[0.9711934]\n",
      "[0.6435598]\n",
      "[0.41050062]\n",
      "[0.43714952]\n",
      "[0.22515807]\n",
      "[0.7706389]\n",
      "[0.54401076]\n",
      "[0.83942336]\n",
      "[0.820051]\n",
      "[0.7668374]\n",
      "[0.55963635]\n",
      "[0.99997807]\n",
      "[0.93810636]\n",
      "[0.58975625]\n",
      "[0.37002894]\n",
      "[0.6398471]\n",
      "[0.510618]\n",
      "[0.91362524]\n",
      "[0.00206175]\n",
      "[0.44231176]\n",
      "[0.5519936]\n",
      "[0.04308026]\n",
      "[0.64539444]\n",
      "[0.86459434]\n",
      "[0.6284951]\n",
      "[0.11605556]\n",
      "[0.1657064]\n",
      "[0.6690309]\n",
      "[0.9414324]\n",
      "[0.60129297]\n",
      "[1.]\n",
      "[0.9958403]\n",
      "[0.6563739]\n",
      "[0.59320873]\n",
      "[0.44106165]\n",
      "[1.]\n",
      "[0.9999957]\n",
      "[0.9217535]\n",
      "[0.34874415]\n",
      "[0.71056867]\n",
      "[0.55416167]\n",
      "[0.21265218]\n",
      "[0.31091735]\n",
      "[0.98553896]\n",
      "[0.6815673]\n",
      "[0.6535208]\n",
      "[0.12224662]\n",
      "[0.60850096]\n",
      "[0.09237475]\n",
      "[0.08652258]\n",
      "[0.30837283]\n",
      "[0.4429169]\n",
      "[0.30356392]\n",
      "[0.63630396]\n",
      "[0.83482724]\n",
      "[0.00909404]\n",
      "[0.8831545]\n",
      "[0.88425046]\n",
      "[0.5102476]\n",
      "[0.8018148]\n",
      "[0.93515825]\n",
      "[0.39462274]\n",
      "[0.07194743]\n",
      "[0.4046469]\n",
      "[0.5445714]\n",
      "[0.42274848]\n",
      "[1.]\n",
      "[0.68191004]\n",
      "[0.9490163]\n",
      "[0.5690988]\n",
      "[0.36630312]\n",
      "[0.08798927]\n",
      "[0.01965969]\n",
      "[0.5009452]\n",
      "[0.9708025]\n",
      "[0.92940617]\n",
      "[0.99574345]\n",
      "[0.03702784]\n",
      "[0.9220775]\n",
      "[0.17586227]\n",
      "[0.9436445]\n",
      "[0.8783243]\n",
      "[0.32047445]\n",
      "[0.8544931]\n",
      "[0.9996706]\n",
      "[0.91321504]\n",
      "[0.98957294]\n",
      "[0.9202577]\n",
      "[0.9101124]\n",
      "[0.37131265]\n",
      "[0.7916413]\n",
      "[0.9539309]\n",
      "[0.26207927]\n",
      "[0.12912062]\n",
      "[0.92727435]\n",
      "[0.98605156]\n",
      "[0.9279529]\n",
      "[0.43599144]\n",
      "[0.47946382]\n",
      "[0.68578434]\n",
      "[0.86273205]\n",
      "[0.9781108]\n",
      "[0.21612024]\n",
      "[0.3781238]\n",
      "[0.95508295]\n",
      "[1.]\n",
      "[0.8392052]\n",
      "[0.6674234]\n",
      "[0.88196546]\n",
      "[0.306341]\n",
      "[0.74363303]\n",
      "[0.422769]\n",
      "[0.9961379]\n",
      "[0.992393]\n",
      "[0.13148388]\n",
      "[0.9999969]\n",
      "[0.8566517]\n",
      "[0.986834]\n",
      "[0.8028078]\n",
      "[0.27502045]\n",
      "[0.89260054]\n",
      "[0.81790847]\n",
      "[0.02270684]\n",
      "[0.89144224]\n",
      "[0.6270168]\n",
      "[0.9676555]\n",
      "[0.8101985]\n",
      "[0.5417029]\n",
      "[0.7024041]\n",
      "[0.2603038]\n",
      "[0.50773364]\n",
      "[0.9997775]\n",
      "[0.99969137]\n",
      "[0.7492135]\n",
      "[0.87052405]\n",
      "[0.10691975]\n",
      "[0.3252347]\n",
      "[0.7217804]\n",
      "[0.64482194]\n",
      "[0.56545883]\n",
      "[0.4709562]\n",
      "[0.85979027]\n",
      "[0.6379368]\n",
      "[0.99913174]\n",
      "[0.99797314]\n",
      "[0.06707899]\n",
      "[0.8266173]\n",
      "[0.9925311]\n",
      "[0.10576018]\n",
      "[0.8204527]\n",
      "[0.4706775]\n",
      "[8.0012964e-05]\n",
      "[0.7710102]\n",
      "[0.99790823]\n",
      "[0.9700024]\n",
      "[0.995827]\n",
      "[0.9726303]\n",
      "[0.9671386]\n",
      "[0.8428014]\n",
      "[0.49553636]\n",
      "[1.]\n",
      "[0.70826775]\n",
      "[0.7768715]\n",
      "[0.9900142]\n",
      "[0.99999213]\n",
      "[0.43478668]\n",
      "[0.06456358]\n",
      "[0.85984355]\n",
      "[0.40777972]\n",
      "[0.36058265]\n",
      "[0.10338959]\n",
      "[0.57465357]\n",
      "[0.14188054]\n",
      "[0.38484117]\n",
      "[0.33569196]\n",
      "[0.24127653]\n",
      "[0.5477982]\n",
      "[0.1598261]\n",
      "[0.9898887]\n",
      "[0.7488863]\n",
      "[0.43387955]\n",
      "[0.5017642]\n",
      "[0.7921283]\n",
      "[0.20814401]\n",
      "[0.0596571]\n",
      "[0.95971084]\n",
      "[0.36676338]\n",
      "[0.99972683]\n",
      "[0.58008647]\n",
      "[0.02701935]\n",
      "[0.9985843]\n",
      "[0.46717128]\n",
      "[0.47980952]\n",
      "[0.50857043]\n",
      "[0.31562102]\n",
      "[0.97721046]\n",
      "[0.45247442]\n",
      "[0.8749501]\n",
      "[0.7240306]\n",
      "[0.06627993]\n",
      "[0.28765422]\n",
      "[0.32015306]\n",
      "[0.6756219]\n",
      "[0.5871696]\n",
      "[0.9702746]\n",
      "[0.99400723]\n",
      "[0.05819746]\n",
      "[0.5157041]\n",
      "[0.40841025]\n",
      "[0.99310654]\n",
      "[0.9867382]\n",
      "[0.7742708]\n",
      "[0.1321578]\n",
      "[0.22820178]\n",
      "[0.96912676]\n",
      "[0.99863654]\n",
      "[0.42733255]\n",
      "[0.9614792]\n",
      "[0.44821724]\n",
      "[0.6624873]\n",
      "[0.31668797]\n",
      "[0.80948275]\n",
      "[0.8470505]\n",
      "[0.98908174]\n",
      "[0.45995733]\n",
      "[0.580249]\n",
      "[0.99237555]\n",
      "[0.32296327]\n",
      "[0.29686213]\n",
      "[0.6600093]\n",
      "[0.51991695]\n",
      "[0.98038656]\n",
      "[0.9999608]\n",
      "[0.9974107]\n",
      "[0.71826345]\n",
      "[0.17249984]\n",
      "[0.82739973]\n",
      "[0.7858787]\n",
      "[0.99942446]\n",
      "[0.633865]\n",
      "[0.9048966]\n",
      "[0.58272654]\n",
      "[0.3958155]\n",
      "[0.27454272]\n",
      "[0.6804924]\n",
      "[0.92789483]\n",
      "[0.7243019]\n",
      "[0.6596016]\n",
      "[0.1833198]\n",
      "[0.03347163]\n",
      "[0.43957475]\n",
      "[0.62404484]\n",
      "[0.222944]\n",
      "[0.9785599]\n",
      "[0.65687937]\n",
      "[0.73488283]\n",
      "[0.6060897]\n",
      "[0.24084878]\n",
      "[1.]\n",
      "[0.794759]\n",
      "[0.9682994]\n",
      "[0.21126084]\n",
      "[0.24826334]\n",
      "[0.08401261]\n",
      "[0.7133119]\n",
      "[0.3365038]\n",
      "[0.80021644]\n",
      "[0.99461764]\n",
      "[0.9700164]\n",
      "[0.04530527]\n",
      "[0.7070313]\n",
      "[0.01960034]\n",
      "[0.0881155]\n",
      "[0.09546405]\n",
      "[0.26733515]\n",
      "[0.74633664]\n",
      "[0.99708015]\n",
      "[0.430299]\n",
      "[0.8508876]\n",
      "[0.7519992]\n",
      "[0.96549493]\n",
      "[0.98507684]\n",
      "[0.00095394]\n",
      "[0.579039]\n",
      "[0.60469925]\n",
      "[0.3028971]\n",
      "[0.9355017]\n",
      "[0.18347101]\n",
      "[0.9995443]\n",
      "[0.4712977]\n",
      "[0.5627907]\n",
      "[0.4152524]\n",
      "[0.78920245]\n",
      "[0.29837826]\n",
      "[0.8572578]\n",
      "[0.44281185]\n",
      "[6.9545026e-13]\n",
      "[0.3599701]\n",
      "[0.47824758]\n",
      "[0.7843374]\n",
      "[0.80422163]\n",
      "[0.39068437]\n",
      "[0.0908456]\n",
      "[0.45993352]\n",
      "[0.8373533]\n",
      "[0.97316384]\n",
      "[0.9693661]\n",
      "[0.90585667]\n",
      "[0.9921794]\n",
      "[0.11506997]\n",
      "[0.65498096]\n",
      "[0.99769837]\n",
      "[0.7956919]\n",
      "[0.7170189]\n",
      "[0.88467354]\n",
      "[0.76169175]\n",
      "[0.5617144]\n",
      "[0.24032193]\n",
      "[0.4309659]\n",
      "[0.14342207]\n",
      "[0.86019105]\n",
      "[0.76903236]\n",
      "[0.9967759]\n",
      "[0.8819945]\n",
      "[0.8040919]\n",
      "[0.2318311]\n",
      "[0.24833284]\n",
      "[0.0965699]\n",
      "[0.7508311]\n",
      "[0.9761496]\n",
      "[0.52669704]\n",
      "[0.75664675]\n",
      "[0.38001755]\n",
      "[0.54310805]\n",
      "[0.9951167]\n",
      "[0.9296184]\n",
      "[0.53379494]\n",
      "[0.99762326]\n",
      "[0.9467883]\n",
      "[0.9998822]\n",
      "[0.92632294]\n",
      "[1.]\n",
      "[0.5235592]\n",
      "[0.24337131]\n",
      "[0.51841384]\n",
      "[0.9763291]\n",
      "[0.9994418]\n",
      "[0.589665]\n",
      "[0.42394516]\n",
      "[0.9039158]\n",
      "[0.999956]\n",
      "[0.9878247]\n",
      "[0.9676953]\n",
      "[0.0392991]\n",
      "[0.4948919]\n",
      "[0.91182363]\n",
      "[0.99897563]\n",
      "[0.9319583]\n",
      "[0.5693398]\n",
      "[0.9443132]\n",
      "[0.74386907]\n",
      "[0.81217444]\n",
      "[0.53738]\n",
      "[0.35234052]\n",
      "[0.00025175]\n",
      "[0.20591553]\n",
      "[0.9541801]\n",
      "[0.8345213]\n",
      "[0.35378066]\n",
      "[0.10117913]\n",
      "[0.03455428]\n",
      "[0.9861995]\n",
      "[0.9088465]\n",
      "[0.61198044]\n",
      "[0.3335055]\n",
      "[0.9946912]\n",
      "[0.7056243]\n",
      "[0.51943654]\n",
      "[0.04149222]\n",
      "[0.05306183]\n",
      "[0.15721495]\n",
      "[0.99936014]\n",
      "[0.8830319]\n",
      "[0.2977095]\n",
      "[0.99997735]\n",
      "[1.]\n",
      "[1.]\n",
      "[0.57587904]\n",
      "[0.93082243]\n",
      "[0.46555963]\n",
      "[0.7617511]\n",
      "[0.6281732]\n",
      "[0.92029667]\n",
      "[0.81015766]\n",
      "[0.94126046]\n",
      "[0.27395126]\n",
      "[0.58742094]\n",
      "[0.27085242]\n",
      "[0.94206107]\n",
      "[0.3870221]\n",
      "[0.99979943]\n",
      "[0.76590854]\n",
      "[0.8087023]\n",
      "[0.983257]\n",
      "[0.29280013]\n",
      "[0.87626934]\n",
      "[0.38408822]\n",
      "[0.60201395]\n",
      "[0.91811156]\n",
      "[0.56402403]\n",
      "[0.42213863]\n",
      "[0.9880824]\n",
      "[0.8157327]\n",
      "[0.99568695]\n",
      "[0.28879014]\n",
      "[0.98524517]\n",
      "[0.8669569]\n",
      "[0.9088223]\n",
      "[0.475798]\n",
      "[0.28319356]\n",
      "[0.22267228]\n",
      "[0.7959909]\n",
      "[0.47050261]\n",
      "[0.33691648]\n",
      "[0.7801968]\n",
      "[0.6916265]\n",
      "[0.26994833]\n",
      "[0.5303753]\n",
      "[0.63591295]\n",
      "[0.9123584]\n",
      "[0.12128919]\n",
      "[0.43761227]\n",
      "[0.90315175]\n",
      "[1.]\n",
      "[0.6150121]\n",
      "[0.525693]\n",
      "[0.20504531]\n",
      "[0.9998884]\n",
      "[0.8011193]\n",
      "[0.97459567]\n",
      "[0.3354797]\n",
      "[0.98278666]\n",
      "[0.2903929]\n",
      "[0.313632]\n",
      "[0.8860738]\n",
      "[0.11936653]\n",
      "[0.05272708]\n",
      "[0.9896276]\n",
      "[0.47796494]\n",
      "[0.9568617]\n",
      "[0.22853608]\n",
      "[0.17224187]\n",
      "[0.9996094]\n",
      "[0.7528063]\n",
      "[0.4454578]\n",
      "[0.4409258]\n"
     ]
    }
   ],
   "source": [
    "output2 = open(\"pred.npy\",'rb')\n",
    "pred_load = np.load(output2)\n",
    "#model.evaluate_generator(test_generator)\n",
    "#from postprocess import postprocess\n",
    "for x in pred_load:\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_class_index(a):\n",
    "    b = {}\n",
    "    for i in range(len(a)):\n",
    "        if a[i] > .5:\n",
    "            b[i] = 1\n",
    "        else:\n",
    "            b[i] = 0\n",
    "    return b\n",
    "def find_class_pred_index(a):\n",
    "    b = {}\n",
    "    for i in range(len(a)):\n",
    "        if a[i][0] > .5:\n",
    "            b[i] = 1\n",
    "        else:\n",
    "            b[i] = 0\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    }
   ],
   "source": [
    "drink_classes = {0:'cat',1:'dog'}\n",
    "test_batch_size = 16\n",
    "epoch = test_generator.samples//test_batch_size+1\n",
    "def check_wrong_picture():\n",
    "    wrong_pictures = []\n",
    "    for i in range(epoch):\n",
    "        x = test_generator.next()\n",
    "        \n",
    "        true_y = x[1]\n",
    "        predict = model.predict_on_batch(x[0])\n",
    "        true_class = find_class_index(true_y)\n",
    "\n",
    "        predict_class = find_class_pred_index(predict)\n",
    "        for j in range(len(x[0])):\n",
    "            true_class[j] = drink_classes[true_class[j]]\n",
    "            predict_class[j] = drink_classes[predict_class[j]]\n",
    "            \n",
    "            if not predict_class[j] == true_class[j]:\n",
    "                #print(true_y[j],predict[j])\n",
    "                wrong_picture = []\n",
    "                wrong_picture.append(predict_class[j])\n",
    "                wrong_picture.append(true_class[j])\n",
    "                wrong_pictures.append(wrong_picture)\n",
    "    return wrong_pictures             \n",
    "wrong_pictures = check_wrong_picture()\n",
    "print(len(wrong_pictures))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
